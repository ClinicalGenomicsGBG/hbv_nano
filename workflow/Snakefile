# Run in this env.: /medstore/Development/nanopore_HBV/daniel/micromamba/envs/snakemake_hbv
# Dry run with: snakemake -np
# Run with e.g.: snakemake -c4

#!/usr/bin/env python3

#testing things

import pandas as pd

global files

configfile: "config/config.yaml"

FILES = glob_wildcards(config["fastq_folder"] + "/{sample}.fastq.gz")    # get all fastq files
NAMES = FILES.sample    # extract sample names

REFERENCE = glob_wildcards("reference_genomes/{ref}.fa")    # get all reference genomes 
REF = REFERENCE.ref    # extract reference genome names


rule all:    # specify the final output files
    input:
        expand(f'{config["output"]}/fastp/report/{{sample}}.html', sample=NAMES),    # fastp report
        #expand("fastp/{sample}_filt.fastq.gz", sample=NAMES),    # filtered reads
        expand(f'{config["output"]}/fastp/{{sample}}_filt.fastq.gz', sample=NAMES),    # filtered reads
        #"output/samtools/minimum_error_rates.csv",    # minimum error rates [bc**,ref_*,*.****]
        f'{config["output"]}/samtools/minimum_error_rates.csv',    # minimum error rates [bc**,ref_*,*.****]
        expand("samtools/{sample}.{ref}.bam", sample=NAMES, ref=REF),    # sorted bam files
        expand("consensus/{sample}.{ref}.fa", sample=NAMES, ref=REF),    # consensus sequences
        expand("consensus/medaka/{sample}.{ref}", sample=NAMES, ref=REF),    # medaka consensus sequences
        expand("freebayes/{sample}.{ref}.vcf", sample=NAMES, ref=REF),    # variant calling
        "samtools/view_stats.csv",    # total mapped reads
        "samtools/view_stats_rt.csv",    # mapped reads of the RT region
        "output/copy_files_done.txt",    # files copied to output folder
        "output/vcf_modifications_done.txt",    # vcf files modified
        "output/qc.csv",    # quality control file


# Preprocess the fastq files
rule fastp:
    input:
        fastq = f'{config["fastq_folder"]}/{{sample}}.fastq.gz'    # get all fastq files
    output: 
        fastq = f'{config["output"]}/fastp/{{sample}}_filt.fastq.gz',    # output filtered reads
        report = f'{config["output"]}/fastp/report/{{sample}}.html'
    singularity:
        config["fastp"]
    shell:
        "fastp -i {input.fastq} -o {output.fastq} -h {output.report}"


# Map the reads to the reference genomes
rule minimap2:
    input:
        ref = "reference_genomes/{ref}.fa",    # all reference genomes
        fastq = f'{config["output"]}/fastp/{{sample}}_filt.fastq.gz'    # filtered reads
    output:
        sam = "minimap2/{sample}.{ref}.sam"   # output sam files
    singularity:
        config["minimap2"]
    shell:
        "minimap2 -ax map-ont {input.ref} {input.fastq} > {output.sam}"   # map reads to reference genomes


# Convert the bam files to samfiles and sort
rule samtools:
    input:
        sam = "minimap2/{sample}.{ref}.sam"    # all sam files
    output:
        bam = "samtools/{sample}.{ref}.bam"    # sorted bam files
    singularity:
        config["samtools"]
    shell:
        "samtools view -bS {input.sam} | samtools sort -o {output.bam}"


# Get the error rates for each sample and reference genome
rule samtools_stats:
    input:
        bam = "samtools/{sample}.{ref}.bam"    # all bam files
    output:
        stats = temp("samtools/{sample}.{ref}.txt")  # output the error rate for each sample and reference genome
    singularity:
        config["samtools"]
    shell:
        "samtools stats {input.bam} |grep -i 'error rate' | cut -f 3 > {output.stats}"    # extract error rates from samtools stats output and write to txt files


# Index the bam files
rule samtools_index:
    input:
        bam = "samtools/{sample}.{ref}.bam"
    output:
        bai = temp("samtools/{sample}.{ref}.bam.bai")
    singularity:
        config["samtools"]
    shell:
        "samtools index {input.bam} {output.bai}"


# Count the total mapped reads and the mapped reads of the RT region
rule samtools_view:
    input:
        bam = "samtools/{sample}.{ref}.bam",
        bai = "samtools/{sample}.{ref}.bam.bai"
    output:
        stats = temp("samtools/{sample}.{ref}.v.txt"),
        mapped_reads = temp("samtools/{sample}.{ref}.rt.txt")
    singularity:
        config["samtools"]
    shell:
        """
        samtools view -F 260 {input.bam} | awk 'length($10) >= 400' | wc -l > {output.stats}   
        samtools view -F 260 {input.bam} {wildcards.ref}:130-1161 | awk 'length($10) >= 400' | wc -l > {output.mapped_reads}
        """


# Collate the total mapped reads from all samples and reference genomes
rule collate_view_stats:
    input:
        expand('samtools/{sample}.{ref}.v.txt', sample=NAMES, ref=REF)
    output:
        "samtools/view_stats.csv"    # all view stats
    run:
        with open(output[0], 'w') as out:
            for path in input:
                sample = path.split('.')[0].split('/')[1]
                ref = path.split('.')[1]

                for view_stat in open(path):
                    out.write(f"{sample},{ref},{view_stat}")    # write view stats to csv file [sample, reference genome, view stat]


# Collate the mapped reads of the RT region from all samples and reference genomes
rule collate_rt_stats:
    input:
        expand('samtools/{sample}.{ref}.rt.txt', sample=NAMES, ref=REF)
    output:
        "samtools/view_stats_rt.csv"    # all view stats
    run:
        with open(output[0], 'w') as out:
            for path in input:
                sample = path.split('.')[0].split('/')[1]
                ref = path.split('.')[1]

                for view_stat in open(path):
                    out.write(f"{sample},{ref},{view_stat}")    # write view stats to csv file [sample, reference genome, view stat]                    


# Collate error rates from all samples and reference genomes
rule collate_error_rates:    
    input:
        expand('samtools/{sample}.{ref}.txt', sample=NAMES, ref=REF)
    output:
        #"samtools/error_rates.csv"    # all error rates
        f'{config["output"]}/samtools/error_rates.csv'
    run:
        with open(output[0], 'w') as out:
            for path in input:
                sample = path.split('.')[0].split('/')[1]
                ref = path.split('.')[1]

                for error in open(path):
                    out.write(f"{sample},{ref},{error}")    # write error rates to csv file [sample, reference genome, error rate]


# Get the minimum error rate for each sample
rule minimum_error_rates:
    input:
        #"samtools/error_rates.csv"
        f'{config["output"]}/samtools/error_rates.csv'
    output:
        #"output/samtools/minimum_error_rates.csv"
        f'{config["output"]}/samtools/minimum_error_rates.csv'
    script:
        "scripts/error_rate.py"                                                                                              


# Generate the consensus sequences (draft assemblies)
rule consensus:
    input:
        bam = "samtools/{sample}.{ref}.bam"
    output:
        fasta = "consensus/{sample}.{ref}.fa"
    singularity:
        config["samtools"]
    shell:
        "samtools consensus -m simple -d20 {input.bam} > {output.fasta}"


# Polish the consensus sequences with medaka
rule medaka:
    input:
        fastq = f'{config["output"]}/fastp/{{sample}}_filt.fastq.gz',
        fasta = "consensus/{sample}.{ref}.fa"
    params:
        model = "r1041_e82_400bps_sup_g615"
    output:
        fasta_medaka = directory("consensus/medaka/{sample}.{ref}")
    singularity:
        config["medaka"]
    shell:
        "medaka_consensus -r N -i {input.fastq} -d {input.fasta} -o {output.fasta_medaka} -m {params.model}"


# Do variant calling
rule freebayes:
    input:
        bam = "samtools/{sample}.{ref}.bam",
        ref = "reference_genomes/{ref}.fa"
    output:
        vcf = "freebayes/{sample}.{ref}.vcf"
    singularity:
        config["freebayes"]
    shell:  
        "freebayes -f {input.ref} {input.bam} > {output.vcf}"


# Copy all files relevant to the clinic to the output folder
rule copy_files:
    input:
        "output/vcf_modifications_done.txt",  
        expand("consensus/medaka/{sample}.{ref}", sample=NAMES, ref=REF)
    output:
        "output/copy_files_done.txt"
    script:
        "scripts/copy_files.py"


# Modify the vcf files and perform calculations 
rule vcf_modification:
    input:
        #"output/samtools/minimum_error_rates.csv",
        f'{config["output"]}/samtools/minimum_error_rates.csv',
        expand("freebayes/{sample}.{ref}.vcf", sample=NAMES, ref=REF)
    output:
        "output/vcf_modifications_done.txt"
    script:
        "scripts/vcf_modification.py"


# Generate a quality control file with check of negative control and genome coverage
rule qc:
    input:
        "output/copy_files_done.txt",
    output:
        "output/qc.csv"
    script:
        "scripts/qc.py"