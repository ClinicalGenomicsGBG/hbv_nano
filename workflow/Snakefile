# Run in this env.: /medstore/Development/nanopore_HBV/daniel/micromamba/envs/snakemake_hbv
# Dry run with: snakemake -np
# Run with e.g.: snakemake -c4
#!/usr/bin/env python3

FILES = glob_wildcards("data/{sample}.fastq.gz")    # get all fastq files
NAMES = FILES.sample    # extract the sample names from the files

REFERENCE = glob_wildcards("reference_genomes/{ref}.fa")    # get all reference genomes 
REF = REFERENCE.ref    # extract the reference genome names

dict = {"sample": [], "ref": [], "error": []}    # create a dictionary with the sample names and reference genomes

rule all:    # specify the final output files
    input:
        expand("fastp/report/{sample}_filt.html", sample=NAMES),   # output fastp report
        "samtools/minimum_error_rates.csv"    # output minimum error rates

rule fastp:    # filter reads
    input:
        fastq = "data/{sample}.fastq.gz"
    output: 
        fastq = "fastp/{sample}_filt.fastq.gz",    # output filtered read
        report = "fastp/report/{sample}_filt.html"       # output fastp report
    log:
        "logs/fastp/{sample}.log"
    shell:
        "fastp -i {input.fastq} -o {output.fastq} -h {output.report}"


rule minimap2:    # map reads to reference genomes.
    input:
        ref = "reference_genomes/{ref}.fa",    # all reference genomes
        fastq = "fastp/{sample}_filt.fastq.gz"    # filtered reads
    output:
        sam = "minimap2/{sample}.{ref}.sam"   # output temporary sam file
    shell:
        "minimap2 -ax map-ont {input.ref} {input.fastq} > {output.sam}"   # map reads to reference genomes


rule samtools:
    input:
        sam = "minimap2/{sample}.{ref}.sam"
    output:
        bam = "samtools/{sample}.{ref}.bam"
    shell:
        "samtools view -bS {input.sam} | samtools sort -o {output.bam}"


rule samtools_stats:
    input:
        bam = "samtools/{sample}.{ref}.bam"
    output:
        stats = "samtools/{sample}.{ref}.txt"  # Creates one txt file for each mapping. Should be improved.
    shell:
        "samtools stats {input.bam} |grep -i 'error rate' | cut -f 3 > {output.stats}"    # extract error rate from samtools stats output


rule collate_error_rates:
    input:
        expand('samtools/{sample}.{ref}.txt', sample=NAMES, ref=REF)
    output:
        "samtools/error_rates.csv"
    run:
        with open(output[0], 'w') as out:
            for path in input:
                sample = path.split('.')[0].split('/')[1]
                ref = path.split('.')[1]

                for error in open(path):
                    out.write(f"{sample},{ref},{error}")


rule minimum_error_rates:
    input:
        "samtools/error_rates.csv"
    output:
        "samtools/minimum_error_rates.csv"
    #shell:
    #    "python3 workflow/scripts/error_rate.py {input} {output}"
    script:
        "scripts/error_rate.py"