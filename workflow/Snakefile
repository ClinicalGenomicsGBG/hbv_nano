# Run in this env.: /medstore/Development/nanopore_HBV/daniel/micromamba/envs/snakemake_hbv
# Dry run with: snakemake -np
# Run with e.g.: snakemake -c4

#!/usr/bin/env python3

import pandas as pd

FILES = glob_wildcards("data/{sample}.fastq.gz")    # get all fastq files
NAMES = FILES.sample    # extract the sample names from the files

REFERENCE = glob_wildcards("reference_genomes/{ref}.fa")    # get all reference genomes 
REF = REFERENCE.ref    # extract the reference genome names

df = pd.read_csv("/medstore/Development/nanopore_HBV/daniel/hbv_nano/samtools/minimum_error_rates_2.csv", header=None)
BAMFILES = df.iloc[:,0].tolist()

#BAMFILES = ["bc13.ref_c.bam", "bc14.ref_d.bam", "bc15.ref_d.bam", "bc16.ref_c.bam", "bc17.ref_d.bam", "bc18_neg_ctrl.ref_c.bam"]    # this should be replaced by reading in the bam files from a csv.

rule all:    # specify the final output files
    input:
        expand("fastp/report/{sample}_filt.html", sample=NAMES),   # output fastp report
        expand("consensus/{bam}.fa", bam=BAMFILES)
        #expand("consensus/{sample}.{ref}.fa", sample=NAMES, ref=REF)    # output consensus sequences

rule fastp:    # filter reads
    input:
        fastq = "data/{sample}.fastq.gz"
    output: 
        fastq = "fastp/{sample}_filt.fastq.gz",    # output filtered read
        report = "fastp/report/{sample}_filt.html"       # output fastp report
    log:
        "logs/fastp/{sample}.log"
    shell:
        "fastp -i {input.fastq} -o {output.fastq} -h {output.report}"


rule minimap2:    # map reads to reference genomes.
    input:
        ref = "reference_genomes/{ref}.fa",    # all reference genomes
        fastq = "fastp/{sample}_filt.fastq.gz"    # filtered reads
    output:
        sam = "minimap2/{sample}.{ref}.sam"   # output temporary sam file
    shell:
        "minimap2 -ax map-ont {input.ref} {input.fastq} > {output.sam}"   # map reads to reference genomes


rule samtools:
    input:
        sam = "minimap2/{sample}.{ref}.sam"
    output:
        bam = "samtools/{sample}.{ref}.bam"
    shell:
        "samtools view -bS {input.sam} | samtools sort -o {output.bam}"


rule samtools_stats:
    input:
        bam = "samtools/{sample}.{ref}.bam"
    output:
        stats = "samtools/{sample}.{ref}.txt"  # Creates one txt file for each mapping. Should be improved.
    shell:
        "samtools stats {input.bam} |grep -i 'error rate' | cut -f 3 > {output.stats}"    # extract error rate from samtools stats output


rule collate_error_rates:
    input:
        expand('samtools/{sample}.{ref}.txt', sample=NAMES, ref=REF)
    output:
        "samtools/error_rates.csv"
    run:
        with open(output[0], 'w') as out:
            for path in input:
                sample = path.split('.')[0].split('/')[1]
                ref = path.split('.')[1]

                for error in open(path):
                    out.write(f"{sample},{ref},{error}")


rule minimum_error_rates:
    input:
        "samtools/error_rates.csv"
    output:
        "samtools/minimum_error_rates.csv",
        "samtools/minimum_error_rates_2.csv"
    script:
        "scripts/error_rate.py"                                                                                              


rule consensus:
    input:
        bam = "samtools/{bam}"
    output:
        fasta = "consensus/{bam}.fa"
    shell:
        "samtools consensus -m simple -d20 {input.bam} > {output.fasta}"


#testing stuff
#VAR = 100

#rule test:
#    #input: "a.in"
#    #output: "a.out"
#    params:
#        var=VAR  # pass the variable value as "var"
#    script: "scripts/test.py"

#print(VAR, "edited")

#def print_var():
#    print(VAR, "edited")
#
#rule print_var:
#    run:
#        print_var()
